---
title: "homework"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{homework}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


## Question

3.3 
The Pareto(a, b) distribution has cdf
                 $$F(x)=1-(\frac{b}{x})^a ,x>=b>0,a>0$$
Derive the probability inverse transformation $F^{-1}(U)$ and use the inverse transform method to simulate a random sample from the Pareto(2, 2) distribution. Graph the density histogram of the sample with the Pareto(2, 2) density superimposed for comparison.

## Answer

```{r}
n <- 1000
u <- runif(n)
x <- 2*(1-u)^(-1/2) # F(x) = 1-(2/x)^2 ,x>=2，so F^-1(u)=2*(1-u)^(-1/2)
hist(x, probability = TRUE, main = expression(f(x)==8*x^(-2)), xlim = c(0,50))#density f(x)==8*x^(-2)
y <- seq(0, 50, .01)
lines(y, 8*y^(-3))#density curve f(x)
```


## Question

3.7 
Write a function to generate a random sample of size n from the Beta(a, b) distribution by the acceptance-rejection method. Generate a random sample
of size 1000 from the Beta(3,2) distribution. Graph the histogram of the
sample with the theoretical Beta(3,2) density superimposed.

## Answer

```{r}
Gen<-function(n,a,b){
  j<-0# the number of experiments for generating n random number
  k<-0
  y<-numeric(n)
  while(k<n){
    u <- runif(1)
    j <- j + 1
    x <- runif(1) #random variate from g(.)
    c<- (a-1)/(a+b-2)#the maximum of beta(a,b) occurs at this point
    c<- beta(a,b)^-1 * c^(a-1) * (1-c)^(b-1)#the maximum of beta(a,b)
    if ( beta(a,b)^-1 * x^(a-1) * (1-x)^(b-1) / c > u) {
      #we accept x
      k <- k + 1
      y[k] <- x
    }
  }
  y
}
y<-Gen(1000, 3, 2)
hist(y,probability = TRUE,main = expression(f(x)==12*x^2 *(1-x)))
a <- seq(0, 1, 0.01)
lines(a, 12*a^2 *(1-a)) #density curve f(x)
```


## Question

3.12
Simulate a continuous Exponential-Gamma mixture. Suppose that the rate
parameter Λ has Gamma(r, β) distribution and Y has Exp(Λ) distribution.
That is, $(Y |Λ = λ) ∼ f_Y (y|λ) = λe^{−λy}$. Generate 1000 random observations from this mixture with r = 4 and β = 2.

## Answer

```{r}
n <- 1e3; r <- 4; beta <- 2
lambda <- rgamma(n, r, beta)#the distribution of parameter
x <- rexp(n, lambda) # mix the distribution
length(x)
head(x)
```


## Question

3.13
 It can be shown that the mixture in Exercise 3.12 has a Pareto distribution with cdf
                  $$F(y)=1-(\frac{\beta}{\beta+y})^r ,r>=0$$
(This is an alternative parameterization of the Pareto cdf given in Exercise3.3.) Generate 1000 random observations from the mixture with r = 4 and β = 2. Compare the empirical and theoretical (Pareto) distributions by graphing the density histogram of the sample and superimposing the Pareto density curve.

## Answer

```{r}
hist(x,probability = TRUE)#graph the histogram of the sample of Exercise 3.12
y <- seq(0, 20, 0.01) 
lines(y, 64/(y+2)^5) #density curve f(x)
```


## Question

1:
• For $n = 10^4, 2 × 10^4, 4 × 10^4, 6 × 10^4, 8 × 10^4$, apply the fast sorting algorithm to randomly permuted
numbers of 1,..., n.
• Calculate computation time averaged over 100 simulations, denoted by $a_n$.
• Regress $a_n$ on $t_n := n log(n)$, and graphically show the results (scatter plot and regression line).
## Answer

```{r}
quick_sort<-function(x){
  num<-length(x)
  if(num==0||num==1){
    return(x)
  }
  else{
    a<-x[1]#pick the number in the first place to compare with all other numbers
    y<-x[-1]#delete the number we pick
    lower<-y[y<a]#the samller numbers
    upper<-y[y>=a]#the larger numbers
    return(c(quick_sort(lower),a,quick_sort(upper)))}#place the smaller numbers to the left of x and the larger numbers to the right. And repeat the procedure recursively to the left numbers and the right numbers separately.
}
n <- c(1e4,2e4,4e4,6e4,8e4)
a_n <- numeric(5)
for (i in 1:5) {
  computation_time <- numeric(100)
  for (j in 1:100) {
    test <- sample(1:n[i])
    computation_time[j] <- system.time(quick_sort(test))[1]
  }
  a_n[i] <- mean(computation_time)
}
t <- n*log(n)
my_lm<-lm(t~a_n)
plot(a_n,t,main='a_n~nlog(n) regression')
abline(my_lm)
```


## Question

5.6
In Example 5.7 the control variate approach was illustrated for Monte Carlo integration of
                   $\theta=\int_0^1e^xdx.$
Now consider the antithetic variate approach. Compute $Cov(e^U , e^{1−U} )$ and $Var(e^U + e^{1−U})$, where $U ∼ Uniform(0,1)$. What is the percent reduction in variance of $\hat\theta$ that can be achieved using antithetic variates (compared with simple MC)?

## Answer
\begin{align*}
E[e^u]=&E[e^{1-u}]= e-1\\
                \\
cov(e^U,e^{1−U})=& E[(e^u-E(e^u))(e^{1-u}-E(e^{1-u}))]\\
                =& E[e^u*e^{1-u}-e^u*E[e^{1-u}]-e^{1-u}*E[e^u]+E[e^]u*E[e^{1-u}]]\\
                =& 3e-1-e^2\\
                \\
var(e^U+e^{1−U})=& var(e^u)+var(e^{1-u})+2cov(e^U,e^{1−U})\\
                =& -5+10e-3e^2\\
                
\end{align*}  
For the simple MC, the variance of $\hat{\theta}$ is $\frac{1}{m}var(e^u)$.
For the antithetic variate approach, the variance of $\hat{\theta}$ is
$\frac{m}{2}\frac{1}{m^2}var(e^u+e^{1-u})$.
So the percent reduction in variance of $\hat{\theta}$ is 
\begin{align*}
pr=&\frac{\frac{1}{m}var(e^u)-\frac{m}{2}\frac{1}{m^2}var(e^u+e^{1-u})}{\frac{1}{m}var(e^u)}\\
  =&\frac{var(e^u)-var(e^u+e^{1-u})}{2var(e^u)}\\
  =&\frac{2e^2-6e+2}{-e^2+4e-3}
\end{align*}  
```{r}
(2*exp(2)-6*exp(1)+2)/(-exp(2)+4*exp(1)-3)
```


## Question

5.7
Refer to Exercise 5.6. Use a Monte Carlo simulation to estimate $\theta$ by the antithetic variate approach and by the simple Monte Carlo method. Compute an empirical estimate of the percent reduction in variance using the antithetic variate. Compare the result with the theoretical value from Exercise 5.6.

## Answer

```{r}
MC.Phi <- function(x, R = 10000, antithetic = FALSE) {
  u <- runif(R/2)
  if (antithetic) v <- 1 - u else v <- runif(R/2)
  u <- c(u, v)
  g <- exp(u) 
  cdf <- mean(g)
  cdf
}
m <- 1000
MC1 <- MC2 <- numeric(m)
x <- 1
for (i in 1:m) {
  MC1[i] <- MC.Phi(x, R = 1000, antithetic = FALSE)
  MC2[i] <- MC.Phi(x, R = 1000, antithetic = TRUE)
}##empirical estimate
(var(MC1)-var(MC2))/var(MC1)
```

The theoretical result from Exercise 5.6 is $\frac{2e^2-6e+2}{-e^2+4e-3}=0.9676701$

## Question

5.13
Find two importance functions $f_1$ and $f_2$ that are supported on $(1,\infty)$ and are 'close' to
      $g(x)=\frac{x^2}{\sqrt{2\pi}}e^{-x^2/2}, x>1$
Which of your two importance functions should produce the smaller variance in estimating
    $\int_{1}^{\infty}\frac{x^2}{\sqrt{2\pi}}e^{-x^2/2}dx$
by importance sampling?Explain.

## Answer

$g(x)=\frac{x^2}{\sqrt{2\pi}}e^{-x^2/2}$ is composed of $\frac{x^2}{\sqrt{2\pi}}$ and $e^{-x^2/2}$, so I find the importance functions separately through them.  And through graph the image of g(x), I find it is similar with normal distribution with mean is 3/2, which is $f_1$.

```{r}
w <- 2
x <- seq(0, 10, .01)
g <- x^2/sqrt(2*pi)*exp(-x^2/2)
f1 <- 1/sqrt(2*pi)*exp(-(x-3/2)^2/2)
f2 <- x^2/sqrt(2*pi)*exp(-1/2)
gs <- c(expression(g(x)==x^2/sqrt(2*pi)*exp(-x^2/2)),
        expression(f[1](x)==1/sqrt(2*pi)*exp(-(x-3/2)^2/2)),
        expression(f[2](x)==x^2/sqrt(2*pi)*exp(-1/2)))
#figure (a)
plot(x, g, type = "l", ylab = "",
     ylim = c(0,1), lwd = w,col=1,main='(A)')
lines(x, f1, lty = 2, lwd = w,col=2)
lines(x, f2, lty = 3, lwd = w,col=3)
legend("topright", legend = gs,
       lty = 1:3, lwd = w, inset = 0.02,col=1:3)

#figure (b)
plot(x, g/f1, type = "l", ylab = "",
     ylim = c(0,1.5), lwd = w, lty = 2,col=2,main='(B)')
lines(x, g/f2, lty = 3, lwd = w,col=3)
legend("topright", legend = gs[-1],
       lty = 2:3, lwd = w, inset = 0.02,col=2:3)
m<-1e6
theta.hat <- se <- numeric(2)
g <- function (x) {
  x^2/sqrt(2*pi)*exp(-x^2/2)
}

x <- rnorm(m, mean = 3/2)
i <- which(x<1)
x[i] <- 3/2
fg <- g(x) / dnorm(x, mean = 3/2)
theta.hat[1] <- mean(fg)
se[1] <- sd(fg)

u <- runif(m)
x <- (3*sqrt(2*pi)*exp(-1/2)*u)^{1/3}
i <- which(x<1)
x[i] <- 1
fg <- g(x)/(x^2/sqrt(2*pi)*exp(-1/2))
theta.hat[2] <- mean(fg)
se[2] <- sd(fg)
rbind(theta.hat, se)
```

The image of $f_1$ is closer  to g than $f_2$, and its se is smaller.



## Question

5.15
Obtain the stratified importance sampling estimate in Example 5.13 and compare it with the result of Example 5.10

## Answer

The importance function is $f(x)=\frac{e^{-x}}{1-e^{-1}}$, so in each subinterval ,the conditional density $f_j$ of X is defined by
 $f_j(x)=\frac{f(x,a_{j-1}<x<a_j)}{P(a_{j-1}<x<a_j)}=\frac{e^{-x}}{e^{-a}-e^{-b}}, a_{j-1}\leq{x}<a_j.$
So the conditional distribution is
 $F_j(x)=\frac{e^{-x}-e^{-a}}{e^{-b}-e^{-a}}$, it will be used to generate ramdon number by the inverse transform method.
 
```{r}
M <- 10000; k <- 5
r <- M/k #replicates per stratum
N <- 50 #number of times to repeat the estimation
est <- matrix(0,N,2)
inverse_transform<-function(n,a,b){  
  u<-runif(n)
  x<--log(exp(-a)-(exp(-a)-exp(-b))*u)
  x
} 
g <- function(x)exp(-x)/(1+x^2)*(x>0)*(x<1)
f <- function(x,a,b)exp(-x)/(exp(-a)-exp(-b))
for (i in 1:N) {
  x <- inverse_transform(M, 0, 1)
  est[i, 1] <- mean(g(x)/f(x, 0, 1))
  T <- numeric(k)
  for(j in 1:k){
    a <- (j-1)/k
    b <- j/k
    xj <- inverse_transform(M/k,a,b)
    T[j] <- mean(g(xj)/f(xj, a, b))
  }
  est[i, 2] <- sum(T)
}
round(apply(est,2,mean),4)
round(apply(est,2,sd),5)

```

The former is the result of the importance sampling, the latter is the result of the stratified importance sampling. We can find the estimate is simillar, while the estimated standeard error of  the stratified importance sampling is smaller.

## Question

6.4
Suppose that $X_1,...,X_n$ are a random sample from a from a lognormal distribution with unknown parameters. Construct a 95% confidence interval for the parameter $\mu$. Use a Monte Carlo method to obtain an empirical estimate of the confidence level.

## Answer

$X_1,...,X_n$ are a random sample from a lognormal distribution, so we consider $Y_k=log(X_k),k=1,...n$, $Y_1,...Y_n$ are a random sample from a normal distribution.
In this case ,the 95% CI of $\mu$ is:
        $[\hat\mu-\hat\sigma*t_{n-1}(0.975)/\sqrt{n},\hat\mu+\hat\sigma*t_{n-1}(0.975)/\sqrt{n}]$
```{r}
n <- 10
m <- 1000
mu.hat <- numeric(m)
mu.se <- numeric(m)
for (i in 1:m) {
  y <- rnorm(n)
  mu.hat[i] <- mean(y)#estimate of mu
  mu.se[i] <- sd(y)#estimate of standard error
}
mean((mu.hat - qt(0.975, n-1) * mu.se / sqrt(n) < 0)&(mu.hat + qt(0.975, n-1) * mu.se / sqrt(n) > 0))#the empirical estimate of the confidence level

```


## Question

6.8
Refer to Example 6.16. Repeat the simulation, but also compute the F test of equal variance, at significance level $\hat\alpha\dot= 0.055$. Compare the power of the Count Five test and F test for small, medium, and large sample sizes. (Recall that the F test is not applicable for non-normal distributions.)

## Answer

```{r}

mu1 <- mu2 <- 0
sigma1 <- 1
sigma2 <- 1.5
m <- 10000
n <- c(20, 200, 2000)#sample sizes
power1 <- power2 <- numeric(3)
count5test <- function(x,y){
  X <- x - mean(x)
  Y <- y - mean(y)
  outx <- sum(X > max(Y)) + sum(X < min(Y))
  outy <- sum(Y > max(X)) + sum(Y < min(X))
  return(as.integer(max(c(outx,outy)) > 5))
}#The function count5test returns the value 1 (reject H0) or 0 (do not reject H0).
for (i in 1:3) {
  power1[i] <- mean(replicate(m,expr = {
    x <- rnorm(n[i],mu1,sigma1)
    y <- rnorm(n[i],mu2,sigma2)
    count5test(x,y)
  }))#count five test
  pvalues <- replicate(m,expr={
    x <- rnorm(n[i],mu1,sigma1)
    y <- rnorm(n[i],mu2,sigma2)
    var.test(x, y, ratio = 1,
             alternative = c("two.sided"),
             conf.level = 0.945, ...)$p.value})
  power2[i] <- mean(pvalues<=0.055)#F test of equal variance, at significance level $\hat\alpha\dot= 0.055$
}
power1#the power of the Count Five test for small, medium, and large sample sizes
power2#the power of F test for small, medium, and large sample sizes
```


## Question

·If we obtain the powers for two methods under a particular simulation setting with 10,000 experiments: say, 0.651 for one method and 0.676 for another method. Can we say the powers are different at 0.05 level?

·What is the corresponding hypothesis test problem?

·Which test can we use? Z-test, two-sample t-test, paired-t test or McNemar test? Why?

·Please provide the least necessary information for hypothesis testing.

## Answer

1.$H_0:power1-power2=0 \leftrightarrow H_a:power1-power2\neq0$ The significant level is 0.05

2.Two-sample t-test will be better.The empirical powers are both limit normal distribution, and we only have one sample of X(power1) and Y(power2). So the statistic is :
$\frac{\sqrt{\frac{n_1n_2}{n_1+n_2}(\overline{X}-\overline{Y})}}{S_{X+Y}}=\frac{(X-Y)/\sqrt2}{S_{X+Y}} \sim t_{n_1+n_2-2}$

3.We need to compute the estimate of the standard error of $n_1$ samples of X and $n_2$ samples of Y.

## Question

7.4
Refer to the air-conditioning data set aircondit provided in the boot package. The 12 observations are the times in hours between failures of airconditioning equipment [63, Example 1.1]:
      3, 5, 7, 18, 43, 85, 91, 98, 100, 130, 230, 487.
Assume that the times between failures follow an exponential model $Exp(\lambda)$. Obtain the MLE of the hazard rate $\lambda$ and use bootstrap to estimate the bias and standard error of the estimate.

## Answer

The log likelihood function of $\lambda$ is
 $l(\lambda|\vec{x})=logL(\lambda|\vec{x})=nlog\lambda-\lambda\sum_{i=0}^{n}x_i$,
 derivation of $\lambda$
 $\frac{\partial l(\lambda|\vec{x})}{\partial x}=\frac{n}{\lambda}-\sum_{i=0}^{n}x_i$
 so the MLE of $\lambda$ is $\hat{\lambda}_{MLE}=\frac{n}{\sum_{i=0}^{n}x_i}$
 
```{r}
set.seed(1234)
library(boot)
x <- aircondit$hours
mle <- function(x) length(x)/sum(x)
B <- 1e4
lambdastar <- numeric(B)
lambda <- mle(x)
for (b in 1:B) {
  xstar <- sample(x, replace = TRUE)
  lambdastar[b] <- mle(xstar)
}
round(c(bias=mean(lambdastar)-lambda,se.boot=sd(lambdastar)),3)

```


## Question

7.5
Refer to Exercise 7.4. Compute 95% bootstrap confidence intervals for the mean time between failures $1/\lambda$ by the standard normal, basic, percentile, and BCa methods. Compare the intervals and explain why they may differ.

## Answer

```{r}
set.seed(1234)
library(boot)
x <- aircondit$hours
boot.lambda <- function(x, i) mean(x[i])
obj <- boot(x, statistic = boot.lambda, R = 999)
ci <- boot.ci(obj, type = c("norm", "basic", "perc", "bca"))
ci
```
The intervels are different. Because base properties of them are different, for example, the standard bootstrap CI based on asymptotic normality, the basic bootstrap CI based on the large sample property, the percentile CI (percent) by assuming $\hat{\theta}^∗|X$ and $\hat{\theta}$ have approximately the same distribution, and so on.





## Question

7.A
Conduct a Monte Carlo study to estimate the coverage probabilities of the standard normal bootstrap confidence interval, the basic bootstrap confidence interval, and the percentile confidence interval. Sample from a normal population and check the empirical coverage rates for the sample mean. Find the proportion of times that the confidence intervals miss on the left, and the porportion of times that the confidence intervals miss on the right.

## Answer

```{r}
set.seed(1234)
mu <- 1
sigma <- 2
n <- 1e4
x <- rnorm(n, mean = mu, sd = sqrt(sigma))
mu.hat <- mean(x)
B <- 1e4
mustar <- numeric(B)
for (b in 1:B) {
  xstar <- sample(x, replace = TRUE)
  mustar[b] <- mean(xstar)
}
se.hat <- sd(mustar)
alpha <- 0.05
probs <- c(alpha/2, 1-alpha/2)
ci.normal <- rev(mu.hat - qnorm(probs)*se.hat) ##standard normal
ci.basic <- 2 * mu.hat - rev(quantile(x = mustar, probs = probs, names = FALSE)) ##basic
ci.percent <- quantile(x = mustar, probs = probs, names = FALSE)##percentile
mc.study <- data.frame(rbind(ci.normal, ci.basic, ci.percent))
colnames(mc.study) <- c("2.5%","97.5%")
mc.study['miss.left'] <- rep.int(0, times = 3)
mc.study['miss.right'] <- rep.int(0, times = 3)
for (b in 1:B) {
  xstar <- sample(x, replace = TRUE)
  mustar <- mean(xstar)
  for (i in 1:3) {
    if(mustar < mc.study[i, 1]){
      mc.study[i, 3] <- mc.study[i, 3] + 1
    }
    else if(mustar > mc.study[i, 2]){
      mc.study[i, 4] <- mc.study[i, 3] + 1
    }
  }
}
mc.study['empirical coverage rate'] <- scales::percent(1 - mc.study[i, 3] / B - mc.study[i, 4] / B, 0.01)
mc.study[, 3] <- scales::percent(mc.study[i, 3] / B, 0.01)
mc.study[, 4] <- scales::percent(mc.study[i, 4] / B, 0.01)
mc.study
```

## Question

7.8
Refer to Exercise 7.7. Obtain the jackknife estimates of bias and standard error of $\hat \theta$.

## Answer

```{r}
library(bootstrap)
x<-scor
boot.prin <- function(x,i){
  lambda <- eigen(cov(x[i,]))$value
  theta <- lambda[1]/sum(lambda)
}
n <- length(x[,1])
theta.hat <- boot.prin(x,1:n)
theta.jack <- numeric(n)
for(i in 1:n){
  theta.jack[i] <- boot.prin(x,(1:n)[-i])
}
bias.jack <- (n-1)*(mean(theta.jack)-theta.hat)
se.jack <- sqrt((n-1)*mean((theta.jack-mean(theta.jack))^2))
round(c(bias.jack = bias.jack, se.jack = se.jack),4)
```


## Question

7.11
In Example 7.18, leave-one-out (n-fold) cross validation was used to select the best fitting model. Use leave-two-out cross validation to compute the models.

## Answer

I use leave-two-out cross validation, so I should choose 2 samples to be validation set. Then I take all the possible choice.

```{r}
library(DAAG); attach(ironslag)
n <- length(magnetic) 
e1 <- e2 <- e3 <- e4 <- numeric(2*n*(n-1))
j<-1
for (i in 1:n) {
  for (k in (1:n)[-i]) {
    y <- magnetic[-c(k,i)]
    x <- chemical[-c(k,i)]
    J1 <- lm(y ~ x)
    yhat1_1 <- J1$coef[1] + J1$coef[2] * chemical[i]
    yhat1_2 <- J1$coef[1] + J1$coef[2] * chemical[k]
    e1[j] <- magnetic[i] - yhat1_1
    e1[j+1] <- magnetic[k] - yhat1_2
    J2 <- lm(y ~ x + I(x^2))
    yhat2_1 <- J2$coef[1] + J2$coef[2] * chemical[i] +
      J2$coef[3] * chemical[i]^2
    yhat2_2 <- J2$coef[1] + J2$coef[2] * chemical[k] +
      J2$coef[3] * chemical[k]^2
    e2[j] <- magnetic[i] - yhat2_1
    e2[j+1] <- magnetic[k] - yhat2_2
    J3 <- lm(log(y) ~ x)
    logyhat3_1 <- J3$coef[1] + J3$coef[2] * chemical[i]
    yhat3_1 <- exp(logyhat3_1)
    logyhat3_2 <- J3$coef[1] + J3$coef[2] * chemical[k]
    yhat3_2 <- exp(logyhat3_2)
    e3[j] <- magnetic[i] - yhat3_1
    e3[j+1] <- magnetic[k] - yhat3_2
    J4 <- lm(log(y) ~ log(x))
    logyhat4_1 <- J4$coef[1] + J4$coef[2] * log(chemical[i])
    yhat4_1 <- exp(logyhat4_1)
    logyhat4_2 <- J4$coef[1] + J4$coef[2] * log(chemical[k])
    yhat4_2 <- exp(logyhat4_2)
    e4[j] <- magnetic[i] - yhat4_1
    e4[j+1] <- magnetic[k] - yhat4_2
    j <- j + 2
  }
}
detach(ironslag)
c(mean(e1^2), mean(e2^2), mean(e3^2), mean(e4^2))
```

The quadratic model is best.


## Question

8.2
Implement the bivariate Spearman rank correlation test for independence [255] as a permutation test. The Spearman rank correlation test statistic can be obtained from function cor with method = "spearman". Compare the achieved significance level of the permutation test with the p-value reported by cor.test on the same samples.

## Answer


```{r}
attach(chickwts)
x <- as.vector(weight[feed == "soybean"])
y <- as.vector(weight[feed == "linseed"])
n <- min(length(x),length(y))
x <- sort(x[1:n])
y <- sort(y[1:n])
z <- c(x,y)
R <- 9999
reps <- numeric(R)
t0 <- cor.test(x,y,method = 'spearman')$estimate
for (i in 1:R) {
  xy <- sample(z)
  x1 <- xy[1:n]
  y1 <- xy[-(1:n)]
  reps[i] <- cor(x1, y1,method = 'spearman')
}
p <- mean(abs(c(t0, reps)) >= abs(t0))
p
cor.test(x,y,method = 'spearman')
```

The p-value is so small that I reject $H_0$.

## Question
9.4
Implement a random walk Metropolis sampler for generating the standard Laplace distribution (see Exercise 3.2). For the increment, simulate from a normal distribution. Compare the chains generated when different variances are used for the proposal distribution. Also, compute the acceptance rates of
each chain.


## Answer
```{r}
set.seed(1234)
Gelman.Rubin <- function(psi) {
  # psi[i,j] is the statistic psi(X[i,1:j])
  # for chain in i-th row of X
  psi <- as.matrix(psi)
  n <- ncol(psi)
  k <- nrow(psi)
  
  psi.means <- rowMeans(psi)     #row means
  B <- n * var(psi.means)        #between variance est.
  psi.w <- apply(psi, 1, "var")  #within variances
  W <- mean(psi.w)               #within est.
  v.hat <- W*(n-1)/n + (B/n)     #upper variance est.
  r.hat <- v.hat / W             #G-R statistic
  return(r.hat)
}
laplace <- function(x1, sd, m){
  f <- function(x){
    1 / 2 * exp(- abs(x))
  }
  x <- numeric(m)
  x[1] <- x1
  k <- 0
  u <- runif(m)
  for (i in 2:m) {
    xt <- x[i-1]
    y <- rnorm(1, xt ,sd)
    num <- f(y) * dnorm(xt, y, sd)
    den <- f(xt) * dnorm(y, xt, sd)
    if (u[i] <= num/den){
      x[i] <- y
    } else {
      x[i] <- xt
      k <- k+1     #y is rejected
    }
  }
  print(paste('sd=', sd, ', x1=', x1, ', acceptance rates=', 1 - k/m))
  return(x)
}
k <- 4          #number of chains to generate
n <- 15000      #length of chains
b <- 1000       #burn-in length
#choose overdispersed initial values
x0 <- c(-10, -5, 5, 10)
```

for variance = 1
```{r}
set.seed(1234)
index <- 5000:5500
sd <- 1
X <- matrix(0, nrow=k, ncol=n)
for (i in 1:k){ 
  X[i, ] <- laplace(x0[i], sd, n)
  }
#par(mfrow=c(2,2))
for (i in 1:k){
  plot(index,X[i, index],type = 'l',ylab = paste("x1 = ", x0[i], sep = ''))
}
#compute diagnostic statistics
psi <- matrix(0, nrow=k, ncol=n)
psi <- t(apply(X, 1, cumsum))
for (i in 1:nrow(psi)){
  psi[i,] <- psi[i,] / (1:ncol(psi))
  
}
  
#plot psi for the four chains
par(mfrow=c(1,1))
for (i in 1:k){
  if(i==1){
    plot((b+1):n,psi[i, (b+1):n],ylim=c(-0.2,0.2), type="l", xlab='Index', ylab=bquote(phi))
  }else{
    lines(psi[i, (b+1):n], col=i)
  }
}
legend('topright', c('x1=-10', 'x1=-5', 'x1=5', 'x1=10'), lty = 1, col = 1:4)
rhat <- rep(0, n)
for (j in (b+1):n){
  rhat[j] <- Gelman.Rubin(psi[,1:j])
}
plot(rhat[(b+1):n], type="l", xlab="", ylab="R")

abline(h=1.2, lty=2)
```

for variance = 2
```{r}
set.seed(1234)
index <- 5000:5500
sd <- 2
X <- matrix(0, nrow=k, ncol=n)
for (i in 1:k){ 
  X[i, ] <- laplace(x0[i], sd, n)
  }
#par(mfrow=c(2,2))
for (i in 1:k){
  plot(index,X[i, index],type = 'l',ylab = paste("x1 = ", x0[i], sep = ''))
}
#compute diagnostic statistics
psi <- matrix(0, nrow=k, ncol=n)
psi <- t(apply(X, 1, cumsum))
for (i in 1:nrow(psi)){
  psi[i,] <- psi[i,] / (1:ncol(psi))
  
}
  
#plot psi for the four chains
par(mfrow=c(1,1))
for (i in 1:k){
  if(i==1){
    plot((b+1):n,psi[i, (b+1):n],ylim=c(-0.2,0.2), type="l", xlab='Index', ylab=bquote(phi))
  }else{
    lines(psi[i, (b+1):n], col=i)
  }
}
legend('topright', c('x1=-10', 'x1=-5', 'x1=5', 'x1=10'), lty = 1, col = 1:4)
rhat <- rep(0, n)
for (j in (b+1):n){
  rhat[j] <- Gelman.Rubin(psi[,1:j])
}
plot(rhat[(b+1):n], type="l", xlab="", ylab="R")

abline(h=1.2, lty=2)
```


## Question
9.7
Implement a Gibbs sampler to generate a bivariate normal chain $(X_t, Y_t)$ with zero means, unit standard deviations, and correlation 0.9. Plot the generated sample after discarding a suitable burn-in sample. Fit a simple linear regression model $Y = \beta_0 + \beta_1X$ to the sample and check the residuals of the model for normality and constant variance.


## Answer
```{r}
set.seed(1234)
#initialize constants and parameters
N <- 5000 #length of chain
burn <- 1000 #burn-in length
X<-array(data = NA, dim = c(4, N, 2)) #the 4 chains, a bivariate sample
rho <- .9 #correlation
mu1 <- 0
mu2 <- 0
sigma1 <- 1
sigma2 <- 1
s1 <- sqrt(1-rho^2)*sigma1
s2 <- sqrt(1-rho^2)*sigma2
###### generate the chain #####


for (j in 1:4) {
  X[j, 1, ] <- c(mu1, mu2) #initialize
  for (i in 2:N) {
    x2 <- X[j, i-1, 2]
    m1 <- mu1 + rho * (x2 - mu2) * sigma1/sigma2
    X[j, i, 1] <- rnorm(1, m1, s1)
    x1 <- X[j, i, 1]
    m2 <- mu2 + rho * (x1 - mu1) * sigma2/sigma1
    X[j, i, 2] <- rnorm(1, m2, s2)
  }
  b <- burn + 1
  x <- X[j, b:N, ]
  cat('Means: ', round(colMeans(x),2), '\n')
  cat('Standard errors: ',round(apply(x,2,sd),2), '\n')
  cat('Correlation coefficients: ', round(cor(x[,1],x[,2]),2), '\n')
  
}


```

We choose the last chain to model the regression
```{r}
plot(x[,1],type='l',col=1,lwd=2,xlab='Index',ylab='Random numbers')
lines(x[,2],col=2,lwd=2)
legend('bottomright',c(expression(X[1]),expression(X[2])),col=1:2,lwd=2)
x <- data.frame(x)
my_lm <- lm(X1 ~ X2, data = x)
par(mfrow=c(1,2))
plot(x,  main = "generated data")
hist(my_lm$residuals, main = "residuals of linear model")
par(mfrow=c(1,1))

```

The $\hat R$ of X1
```{r}
#compute diagnostic statistics
X1 <- X[ , , 1]
psi1 <- t(apply(X1, 1, cumsum))
for (i in 1:nrow(psi1)){
  psi1[i,] <- psi1[i,] / (1:ncol(psi1))
  
}

#plot psi for the four chains
par(mfrow=c(1,1))
for (i in 1:4){
  if(i==1){
    plot((b+1):N,psi1[i, (b+1):N],ylim=c(-0.2,0.2), type="l", xlab='Index', ylab=bquote(phi))
  }else{
    lines(psi1[i, (b+1):N], col=i)
  }
}
rhat1 <- rep(0, N)
for (j in (b+1):N){
  rhat1[j] <- Gelman.Rubin(psi1[,1:j])
}
plot(rhat1[(b+1):N], type="l", xlab="", ylab="R")

abline(h=1.2, lty=2)
```

The $\hat R$ of X2
```{r}
#compute diagnostic statistics
X2 <- X[ , , 2]
psi2 <- t(apply(X2, 1, cumsum))
for (i in 1:nrow(psi2)){
  psi2[i,] <- psi2[i,] / (1:ncol(psi2))
  
}

#plot psi for the four chains
par(mfrow=c(1,1))
for (i in 1:4){
  if(i==1){
    plot((b+1):N,psi2[i, (b+1):N],ylim=c(-0.2,0.2), type="l", xlab='Index', ylab=bquote(phi))
  }else{
    lines(psi2[i, (b+1):N], col=i)
  }
}
rhat2 <- rep(0, N)
for (j in (b+1):N){
  rhat2[j] <- Gelman.Rubin(psi2[,1:j])
}
plot(rhat2[(b+1):N], type="l", xlab="", ylab="R")

abline(h=1.2, lty=2)

```

## Question 

Test the intermediary effect

## Answer

```{r}
n <- 10
x <- runif(n, 0, 10)
alpha <- c(0, 0, 1)
beta <- c(0, 1, 0)
gamma <- c(1, 1, 1)
paramas<-cbind(alpha, beta, gamma)
a_m <- 1
a_y <- 1
M<-a_m+x%*%t(alpha)+rnorm(n*3,0,0.1)
Y<-a_y+M%*%diag(beta)+x%*%t(gamma)+rnorm(n*3,0,0.1)
p <- numeric(3)
R <- 999;K <- 1:(2*n)
for(i in 1:3){
  alpha_hat <- numeric(R)
  fit <- lm(M[,i]~x)
  fit <- summary(fit)
  alpha_0 <- fit$coefficients[2,1]
  z <- c(M[,i], x)
  for (j in 1:R) {
    k <- sample(K, size = n, replace = FALSE)
    Mj <- z[k]; xj<- z[-k] 
    fit <- lm(Mj~xj)
    fit <- summary(fit)
    alpha_hat[j] <- fit$coefficients[2,1]
  }
  p[i]<-mean(abs(c(alpha_0, alpha_hat)) >= abs(alpha_0))
}
p ##p-value
```

For (1)$\alpha = 0, \beta = 0$, (2)$\alpha = 0, \beta = 1$, we accept $H_0$. For (3)$\alpha = 1, \beta = 0$, we refuse $H_0$

## QUestion

Consider model $P(Y=1|X_1,X_2,X_3)=expit(a+b_1x_1+b_2x_2+b_3x_3), X_1~P(1),X_2~Exp(1),X_3~B(1,0.5)$


## Answer

(1)
```{r}
f <- function(N, b1, b2, b3, f0){
  x1 <- rpois(N, 1)
  x2 <- rpois(N, 1)
  x3 <- rbinom(N, 1, 0.5)
  P <- function(alpha){
    p <- 1/(1+exp(-alpha-b1*x1-b2*x2-b3*x3))
    mean(p) - f0
  }
  solu <- uniroot(P, c(-20, 20))
  return(solu$root)
}
```

(2)
```{r}
N <- 1e6
b1 <- 0
b2 <- 1
b3 <- -1
f0 <- c(0.1,0.01,0.001,0.0001)
alpha<-numeric(4)
for (i in 1:4){
  alpha[i] <- f(N,b1,b2,b3,f0[i])
}
```

(3)
```{r}
plot(f0,alpha)
```

## Question
after class exercise

## Answer
(1)
MLE:

The likelihood function of observed data is
$L(\lambda)=\prod_{i=1}^nP_{\lambda}(u_i\leq x \leq v_i)=\prod_{i=1}^n[e^{-\lambda u_i}-e^{-\lambda v_i}]$

The log_likelihood function is
$l(\lambda)=\sum_{i=1}^nlog(e^{-\lambda u_i}-e^{-\lambda v_i})$

We can use the mle() to solve it.

EM:

We add up the cpmplete data $X_i$.

For the E-step

$l_0(\lambda;u,v)=E_{\lambda_0}[log L(\lambda;u, x, v)|(u,v)]=n log\lambda-\lambda\sum_{i=1}^nE[x_i|u_i,v_i,\lambda^{(0)}]$

$X_i\sim EXP(\lambda)$ and within interval$[u_i,v_i]$

So 

$f(x_i|u_i,v_i,\lambda_0)=\frac{\lambda e^{-\lambda x_i}}{e^{-\lambda_0 u_i}-e^{-\lambda_0 v_i}}$

then 

$E[x_i|u_i,v_i,\lambda^{(0)}]=\int_{u_i}^{v_i}f(x_i|u_i,v_i,\lambda_0)dx=\frac{1}{\lambda_0}(1+\frac{\lambda_0u_ie^{-\lambda_0 u_i}-\lambda_0v_ie^{-\lambda_0 v_i}}{e^{-\lambda_0 u_i}-e^{-\lambda_0 v_i}})$

For the M_step

maximize $l_0(\lambda;u,v)$,  let the resultant maximizer be $\lambda_1$.

$\lambda_1=\frac{n}{\sum_{i=1}^{n}\frac{1}{\lambda_0}(1+\frac{\lambda_0u_ie^{-\lambda_0 u_i}-\lambda_0v_ie^{-\lambda_0 v_i}}{e^{-\lambda_0 u_i}-e^{-\lambda_0 v_i}})}$

Then, replace $\lambda_0$ with $\lambda_1$, and repeat this process until convergence (iteration).

(2)
MLE
```{r}
u <- c(11,8,27,13,16,0,23,10,24,2)
v <- c(12,9,28,14,17,1,24,11,25,3)
score_function <- function(lambda){
  sum((u*exp(-lambda*u)-v*exp(-lambda*v))/(exp(-lambda*u)-exp(-lambda*v)))
}
# solve directly
solution <- uniroot(score_function,c(0,10))
solution$root
```

EM
```{r}
EM <- function(u, v, max.it=10000, eps=1e-5){
  lambda <- 0.5
  i <- 1
  lambda1 <- 1
  lambda2 <- 0.5
  x <- (1+(lambda*u*exp(-lambda*u)-lambda*v*exp(-lambda*v))/(exp(-lambda*u)-exp(-lambda*v))) / lambda
  while( abs(lambda1 - lambda2) >= eps){
    lambda1 <- lambda2
    lambda2 <- 1/mean(x)
    x <- (1+(lambda2*u*exp(-lambda2*u)-lambda2*v*exp(-lambda2*v))/(exp(-lambda2*u)-exp(-lambda2*v))) / lambda2
    if(i == max.it) break
    i <- i + 1    
  }
  return(lambda2)
}
EM(u,v)
```

We can find that the numeric solutions of the two method is almost same.


## Question
Why do you need to use unlist() to convert a list to an atomic
vector? Why doesn’t as.vector() work?

## Answer 
The factor of list is heterogeneous, so we need to use unlist(), as.factor() works on homogeneous factor.

## Question
Why is 1 == "1" true? Why is -1 < FALSE true? Why is "one"
< 2 false

## Answer 
Although "1" is a charactor, it's value still means 1 in R.
The value of FAlSE is 0, so -1 < FALSE is true.
The values of "one" mean the combind of "o", "n", "e", instead of "1", so "one" < 2 is false.


## Question
What does dim() return when applied to a vector?

## Answer 
```{r}
x <- c(1,2,3,4)
is.vector(x)
dim(x)
```

Vector only has length, it doesn't have dimension.

## Question
If is.matrix(x) is TRUE, what will is.array(x) return?

## Answer 
```{r}
x <- array(c(1,2,3,4),c(2,2))
is.matrix(x)
is.array(x)
```

Because matrix is 2-dimensional array.

## Question
What attributes does a data frame possess?

## Answer 
Data frame is a special 2-dimensioal list, each column has a common name, and each column has the same data type, but different column may has different data type.


## Question
What does as.matrix() do when applied to a data frame with
columns of different types?

## Answer 
```{r}
a <- data.frame(
  name = c("a", "b"),
  number = c("001","002"),
  value = c(1000, 2000)
  
)
as.matrix(a)
```

The type of each column become the same, follow the order logical < integer < double < character < list.

## Question
Can you have a data frame with 0 rows? What about 0
columns?

## Answer 
```{r}
a <- data.frame(
  name = c("a", "b"),
  number = c("001","002"),
  value = c(1000, 2000)
)
b <- a[-c(1,2),]
b
c <- a[,-c(1,2,3)]
c
```

## Question

The function below scales a vector so it falls in the range [0, 1]. How would you apply it to every column of a data frame? How would you apply it to every numeric column in a data frame?

scale01 <- function(x) {
  rng <- range(x, na.rm = TRUE)
  (x - rng[1]) / (rng[2] - rng[1])
}

## Answwr

```{r}
scale01 <- function(x) {
  rng <- range(x, na.rm = TRUE)
  (x - rng[1]) / (rng[2] - rng[1])
}
df <- data.frame(x = 1:4, y = 3:6, z = 6:9)
df
apply(df, 2, scale01)
df <- data.frame(x = 1:4, y = 3:6, z = 6:9, a=c('a', 'b', 'c', 'd'))
is_num <- vapply(df, is.numeric, numeric(1))
ind <- which(is_num == 1)
apply(df[, ind], 2, scale01)
```



## Question

Use vapply() to:
a) Compute the standard deviation of every column in a numeric data frame.
b) Compute the standard deviation of every numeric column in a mixed data frame. (Hint: you’ll need to use vapply() twice.)

## Answer

a)
```{r}
df <- data.frame(x = 1:4, y = 3:6, z = 6:9)
df
vapply(df,sd,FUN.VALUE=('a'=0))
```

b)
```{r}
df <- data.frame(x = 1:4, y = 3:6, z = 6:9, a=c('a', 'b', 'c', 'd'))
df
is_num <- vapply(df, is.numeric, numeric(1))
ind <- which(is_num == 1)
vapply(df[, ind], sd, FUN.VALUE = ('a'=0))
```



## Question

3. Implement a Gibbs sampler to generate a bivariate normal chain $(X_t, Y_t)$ with zero means, unit standard deviations, and correlation 0.9.
• Write an Rcpp function.
• Compare the corresponding generated random numbers with pure R language using the function “qqplot”.
• Compare the computation time of the two functions with the function “microbenchmark”.

## Answer
```{r}
library(Rcpp)
#// This is the giblC.cpp
#include <Rcpp.h>
#include <stdlib.h>
#include <math.h>
#using namespace Rcpp;
#// [[Rcpp::export]]
cppFunction('NumericMatrix gibbsC(){
  int N = 5000;
  NumericMatrix mat(N,2);
  float mu1 = 0;
  float mu2 = 0;
  float sigma1 = 1;
  float sigma2 = 1;
  float rho = 0.9;
  float m1, m2, x1, x2;
  float s1 = sqrt(1-pow(rho,2))*sigma1;
  float s2 = sqrt(1-pow(rho,2))*sigma2;
  mat(0,0)=mu1;
  mat(0,1)=mu2;
  for(int i=1;i<N;i++){
    x2 = mat(i-1, 1);
    m1 = mu1 + (rho * (x2 - mu2) * sigma1/sigma2);
    mat(i,0) = ::Rf_rnorm(m1, s1);
    x1 = mat(i,0);
    m2 = mu2 + (rho * (x1 - mu1) * sigma2/sigma1);
    mat(i,1) = ::Rf_rnorm(m2, s2);
  }
  return mat;
}')
X_Cpp <- gibbsC()
colMeans(X_Cpp)
apply(X_Cpp, 2, sd)
cor(X_Cpp[,1], X_Cpp[,2])
```

```{r}
gibbsR <- function(){
  #initialize constants and parameters
  N <- 5000 #length of chain
  burn <- 1000 #burn-in length
  X <- matrix(0, N, 2) #the chain, a bivariate sample
  rho <- .9 #correlation
  mu1 <- 0
  mu2 <- 0
  sigma1 <- 1
  sigma2 <- 1
  s1 <- sqrt(1-rho^2)*sigma1
  s2 <- sqrt(1-rho^2)*sigma2
  ###### generate the chain #####
  
  X[1, ] <- c(mu1, mu2) #initialize
  for (i in 2:N) {
    x2 <- X[i-1, 2]
    m1 <- mu1 + rho * (x2 - mu2) * sigma1/sigma2
    X[i, 1] <- rnorm(1, m1, s1)
    x1 <- X[i, 1]
    m2 <- mu2 + rho * (x1 - mu1) * sigma2/sigma1
    X[i, 2] <- rnorm(1, m2, s2)
  }
  return(X)
}
X_R <- gibbsR()
colMeans(X_R)
apply(X_R, 2, sd)
cor(X_R[,1], X_R[,2])
qqplot(X_R[, 1], X_R[, 2], main = 'R')
qqplot(X_Cpp[, 1], X_Cpp[, 2], main = 'Cpp')
```

```{r}
library(microbenchmark)
ts <- microbenchmark(X_Cpp <- gibbsC(), X_R <- gibbsR())
summary(ts)[,c(1,3,5,6)]
```

